---
title: Workshop on Meta-Learning (MetaLearn 2022)
description: "@NeurIPS 2022 <br> 2nd December 2022 <br> (Hybrid)"
permalink: /2022/index.html
weight: -1
redirect_from: /2022/
---

### Abstract

Recent years have seen rapid progress in meta-learning methods, which transfer knowledge across tasks and domains to efficiently learn new tasks, optimize the learning process itself, and even generate new learning methods from scratch. Meta-learning can be seen as the logical conclusion of the arc that machine learning has undergone in the last decade, from learning classifiers, to learning representations, and finally to learning algorithms that themselves acquire representations, classifiers, and policies for acting in environments. In practice, meta-learning has been shown to yield new state-of-the-art automated machine learning methods, novel deep learning architectures, and substantially improved one-shot learning systems. Moreover, improving one’s own learning capabilities through experience can also be viewed as a hallmark of intelligent beings, and neuroscience shows a strong connection between human and reward learning and the growing sub-field of meta-reinforcement learning. 

Some of the fundamental questions that this workshop aims to address are:

- What are the meta-learning processes in nature (e.g., in humans), and how can we take inspiration from them?
- What is the relationship between meta-learning, continual learning, and transfer learning?
- What interactions exist between meta-learning and large pretrained / foundation models? 
- What principles can we learn from meta-learning to help us design the next generation of learning systems?
- What kind of theoretical principles can we develop for meta-learning? 
- How can we exploit our domain knowledge to effectively guide the meta-learning process and make it more efficient?
- How can be design better benchmarks for different meta-learning scenarios?

As prospective participants, we primarily target machine learning researchers interested in the questions and foci outlined above. Specific target communities within machine learning include, but are not limited to: meta-learning, AutoML, reinforcement learning, deep learning, optimization, evolutionary computation, and Bayesian optimization. We also invite submissions from researchers who study human learning and neuroscience, to provide a broad and interdisciplinary perspective to the attendees.

### Invited Speakers

<!-- Submit challenge questions for the speakers [here](https://forms.gle/DGEev5erxAmoi6eEA).  -->

TBA

<!--
## Spotlights
### Morning Session
- [**Title**.](slides/metalearn2020-paper.pdf)
 *Authors*

### Afternoon Session
- [**Title**.](slides/metalearn2020-paper.pdf)
 *Authors*
-->

### Organizers

- [Fábio Ferreira](https://ml.informatik.uni-freiburg.de/people/ferreira/index.html) (Freiburg University)
- [Qi Lei](https://cecilialeiqi.github.io/) (Princeton University)
- [Eleni Triantafillou](https://www.cs.toronto.edu/~eleni/) (Google Brain)
- [Joaquin Vanschoren](http://www.win.tue.nl/~jvanscho/) (Eindhoven University of Technology)
- [Huaxiu Yao](https://huaxiuyao.mystrikingly.com/) (Stanford)

## Accepted Papers

Papers accepted to the workshop (sorted in ascending order of submission id). Links will work after the CRC deadline on Nov 20th.

- [HARRIS: Hybrid Ranking and Regression Forests for Algorithm Selection](https://openreview.net/forum?id=Rj6hpOeTyL)
- [Interpolating Compressed Parameter Subspaces](https://openreview.net/forum?id=Zb9m4idh8I)
- [Multiple Modes for Continual Learning](https://openreview.net/forum?id=r0EQ0nJRpp)
- [Topological Continual Learning with Wasserstein Distance and Barycenter](https://openreview.net/forum?id=KadONYTsEv)
- [FiT: Parameter Efficient Few-shot Transfer Learning](https://openreview.net/forum?id=F3N4XrLCCm)
- [Betty: An Automatic Differentiation Library for Multilevel Optimization](https://openreview.net/forum?id=v-aZuDOqQIQ)
- [Meta-Learning General-Purpose Learning Algorithms with Transformers](https://openreview.net/forum?id=t6tA-KB4dO)
- [Conditional Neural Processes for Molecules](https://openreview.net/forum?id=R1VFXrmVRq)
- [Contextual Squeeze-and-Excitation](https://openreview.net/forum?id=0KTEHivEy1)
- [Meta-learning of Black-box Solvers Using Deep Reinforcement Learning](https://openreview.net/forum?id=9pO8hSVu0J)
- [Efficient Queries Transformer Neural Processes](https://openreview.net/forum?id=_3FyT_W1DW)
- [GramML: Exploring Context-Free Grammars with Model-Free Reinforcement Learning](https://openreview.net/forum?id=OpdayUqlTG)
- [Debiasing Meta-Gradient Reinforcement Learning by Learning the Outer Value Function](https://openreview.net/forum?id=HFVmkIRJ4J)
- [MARS: Meta-learning as score matching in the function space](https://openreview.net/forum?id=hV0YobaZ0xp)
- [Meta-Learning via Classifier(-free) Guidance](https://openreview.net/forum?id=fY5xpJszW2-)
- [Neural Architecture for Online Ensemble Continual Learning](https://openreview.net/forum?id=TLsUITLdKKe)
- [Meta-RL for Multi-Agent RL: Learning to Adapt to Evolving Agents](https://openreview.net/forum?id=0toY1f8-Iq9)
- Lightweight Prompt Learning with General Representation for Rehearsal-free Continual Learning
- Uncertainty-Aware Meta-Learning for Multimodal Task Distributions
- Unsupervised Meta-learning via Few-shot Pseudo-supervised Contrastive Learning
- Multi-objective Tree-structured Parzen Estimator Meets Meta-learning
- Few-Shot Calibration of Set Predictors via Meta-Learned Cross-Validation-Based Conformal Prediction
- On the Importance of Architectures and Hyperparameters for Fairness in Face Recognition
- HyperSound: Generating Implicit Neural Representations of Audio Signals with Hypernetworks
- Towards Discovering Neural Architectures from Scratch
- The Curse of Low Task Diversity: On the Failure of Transfer Learning to Outperform MAML and Their Empirical Equivalence
- PriorBand: HyperBand + Human Expert Knowledge
- Expanding the Deployment Envelope of Behavior Prediction via Adaptive Meta-Learning
- GraViT-E: Gradient-based Vision Transformer Search with Entangled Weights
- Learning to Prioritize Planning Updates in Model-based Reinforcement Learning
- One-Shot Optimal Design for Gaussian Process Analysis of Randomized Experiments
- Towards Automated Design of Bayesian Optimization via Exploratory Landscape Analysis
- Recommendation for New Drugs with Limited Prescription Data
- Bayesian Optimization with a Neural Network Meta-learned on Synthetic Data Only
- PersA-FL: Personalized Asynchronous Federated Learning
- AutoRL-Bench 1.0
- Gray-Box Gaussian Processes for  Automated Reinforcement Learning
- Transfer NAS with Meta-learned Bayesian Surrogates
- Optimistic Meta-Gradients
- Achieving a Better Stability-Plasticity Trade-off via Auxiliary Networks in Continual Learning
- Adversarial Cheap Talk
- Efficient Bayesian Learning Curve Extrapolation using Prior-Data Fitted Networks
- Meta-Learning Makes a Better Multimodal Few-shot Learner
- Test-time adaptation with slot-centric models
- LOTUS: Learning to learn with Optimal Transport in Unsupervised Scenarios


## Program

### Schedule

TBA

### Important Dates

- Submission deadline: October 3rd, 2022, 17:00 CEST
- Author Notification: October 20th, 2022, AoE
- SlidesLive recording deadline: November 10th, 2022, AoE
- Camera-ready paper submission deadline: November 20th, 2022, AoE
- Poster submission deadline: November 25th, 2022, AoE

### Formatting

We have provided a modified `.sty` file [here](neurips_2022.sty) that appropriately lists the name of the workshop when `\neuripsfinal` is enabled. Please use this style file in conjunction with the corresponding LaTeX `.tex` template from the [NeurIPS website](https://neurips.cc/Conferences/2022/PaperInformation/StyleFiles) to submit a final camera-ready copy. Both the submission and the camera-ready can be up to **4 pages** (excluding acknowledgements, references and appendices).

[Start your submission](https://openreview.net/group?id=NeurIPS.cc/2022/Workshop/MetaLearn).

### Publication

Accepted papers and supplementary material will be made available on the workshop website. However, these do not constitute archival publications and no formal workshop proceedings will be made available, meaning contributors are free to publish their work in archival journals or conferences.

### FAQ

1. Can supplementary material be added beyond the 4-page limit for submissions, and are there any restrictions on it?

   Yes, you may include additional supplementary material, but you should ensure that the main paper is self-contained, since looking at supplementary material is at the discretion of the reviewers. The supplementary material should also follow the same NeurIPS format as the paper.
   
1. Are references included in the 4-page limit?
   
   No, references will not count towards the page limit.

1. Can a submission to this workshop be submitted to another NeurIPS workshop in parallel?

   We discourage this, as it leads to more work for reviewers across multiple workshops. Our suggestion is to pick one workshop to submit to.

1. Can a paper be submitted to the workshop that has already appeared at a previous conference with published proceedings?

   We won't be accepting such submissions unless they have been adapted to contain significantly new results (where novelty is one of the qualities reviewers will be asked to evaluate).

1. Can a paper be submitted to the workshop that is currently under review or will be under review at a conference during the review phase?

   From our side, it is perfectly fine to submit a condensed version of a parallel conference submission if it is also fine for the conference in question. Our workshop does not have archival proceedings, and therefore parallel submissions of extended versions to other conferences are acceptable.
  
## Review Process

tl;dr: The review process will be **double-blind**. Please sign up to be, or recommend, a reviewer via [https://forms.gle/EW4icbYv5uA8A13KA](https://forms.gle/EW4icbYv5uA8A13KA).

### Important Dates

- Assignments of reviewers to papers (start of review phase): October 4th, 2022
- **Final reviewing deadline**: October 18th, 2022, AoE

### Reviewing Guidelines
We encourage all reviewers to check [our reviewing guidelines](https://docs.google.com/document/d/1vRdY8e2ttALw_kzxviejrhkVkiJlJ7YiS3yGm9jykB0/edit). 

<!--
## Accepted Papers ##

- [**Title**.](papers/paper.pdf)
 *Authors*
-->


## Program Committee

We thank the program committee (senior and junior reviewers) for shaping the excellent technical program; they are (in alphabetical order):

Aaron Klein, 
Abhishek Gupta, 
Alexander Tornede, 
Ana Carolina Lorena, 
Andre Carlos Ponce de Leon Ferreira De Carvalho, 
Andrei Alex Rusu, 
Ang Li, 
Aniruddh Raghu,
Ashvin Nair, 
Benjamin Eysenbach, 
Bingjun Li,
Boris Knyazev,
Bradly C. Stadie,
Chunhui Zhang,
Cuong Quoc Nguyen,
Da Kuang,
Daniel Hernández-Lobato,
Eleni Triantafillou, 
Erin Grant,
Haoyu Wang,
Haozhu Wang,
Huaxiu Yao,  
Ishita Dasgupta, 
Jake Snell, 
Jasmin Bogatinovski, 
Jiajun Wu, 
Jiani Huang, 
Jiaqi Wang, 
John Willes, 
Kate Rakelly, 
Lars Kotthoff, 
Lazar Atanackovic, 
Li Zhong, 
Lin Qiu, 
Louis Kirsch,
Marc Pickett, 
Massimiliano Patacchiola, 
Matthias Feurer, 
Maximilian Igl, 
Mehrtash Harandi, 
Mengye Ren, 
Micah Goldblum, 
Mihai Suteu, 
Mikhail Mekhedkin Meskhi,
Minxue Jia, 
Muchao Ye, 
M. Taha Toghani, 
Ondrej Bohdal, 
Parminder Bhatia, 
Parsa Mahmoudieh, 
Philip Fradkin, 
Piotr W Mirowski, 
Praneet Dutta, 
Quentin Bouniot,
Randal S. Olson, 
Sharare Zehtabian, 
Shengpu Tang, 
Shibo Li, 
Shixun Wu,
Sihong He,
Sreejan Kumar, 
Sungryull Sohn,
Thomas Elsken, 
Tian Xia, 
Tingfeng Li,
Udayan Khurana,
Weihao Song,
Weiran Lin, 
Xueying Ding, 
Yao Su, 
Yawen Wu,
Yihao Xue, 
Ying Wei, 
Yue Tan, 
Yuhong Li, 
Yuhui Zhang,
Yuxin Tang, 
Yu-Xiong Wang,  
Zhenmei Shi, 
Zhepeng Wang and
Zuhui Wang.

## Past Workshops

[Workshop on Meta-Learning (MetaLearn 2021) @ NeurIPS 2021](https://meta-learn.github.io/2021/)

[Workshop on Meta-Learning (MetaLearn 2020) @ NeurIPS 2020](https://meta-learn.github.io/2020/)

[Workshop on Meta-Learning (MetaLearn 2019) @ NeurIPS 2019](https://meta-learn.github.io/2019/)

[Workshop on Meta-Learning (MetaLearn 2018) @ NeurIPS 2018](https://meta-learn.github.io/2018/)

[Workshop on Meta-Learning (MetaLearn 2017) @ NeurIPS 2017](https://meta-learn.github.io/2017/)

<!--
## Sponsors

We are grateful for the support of our sponsors, which enabled us to offer XX to several participants.
-->

## Contacts

For any further questions, you can contact us at <metalearn2022@googlegroups.com>.
